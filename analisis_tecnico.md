# Auditor√≠a T√©cnica ‚Äî MaravIA Gateway

> **Rol del auditor:** Senior Go Engineer & Backend Architect, especializado en API Gateways de alta concurrencia, resiliencia, observabilidad y optimizaci√≥n de CPU/memoria/red.
>
> **Fecha:** 2026-02-22
>
> **Versi√≥n auditada:** commit `60d6056` (branch `main`)

---

## √çndice

1. [Resumen Ejecutivo](#1-resumen-ejecutivo)
2. [Arquitectura Actual](#2-arquitectura-actual)
3. [Hallazgos y Problemas](#3-hallazgos-y-problemas)
   - [üî¥ Cr√≠tico](#-cr√≠tico)
   - [üü° Medio](#-medio)
   - [üü¢ Mejora](#-mejora)
4. [Riesgos de Producci√≥n](#4-riesgos-de-producci√≥n)
5. [Optimizaci√≥n de Performance](#5-optimizaci√≥n-de-performance)
6. [Resiliencia](#6-resiliencia)
7. [Observabilidad](#7-observabilidad)
8. [Checklist de Producci√≥n](#8-checklist-de-producci√≥n)
9. [Score de Madurez](#9-score-de-madurez)

---

## 1. Resumen Ejecutivo

El gateway est√° bien estructurado para un proyecto inicial: layout est√°ndar Go, cliente HTTP compartido, circuit breaker por agente, m√©tricas Prometheus, timeouts de servidor configurables y cierre graceful. **La base es s√≥lida.**

Sin embargo, hay problemas de producci√≥n reales:

- Una **race condition de timeouts** que provoca goroutines zombies bajo carga.
- **Transporte HTTP suboptimizado** (sin `ResponseHeaderTimeout`, sin `MaxConnsPerHost`).
- **Ausencia de backpressure** (sin l√≠mite de concurrencia por agente).
- **CORS mal configurado** (`Allow-Credentials: true` con wildcard).
- **Sin autenticaci√≥n** de requests entrantes.
- **Gaps en observabilidad** (sin correlation ID, sin m√©tricas de circuit breaker state, sin tracing).

**Score actual: 6.2 / 10.** Con las correcciones cr√≠ticas y medias listadas puede llegar a **8.5+**.

---

## 2. Arquitectura Actual

### Flujo de datos

```
n8n
 ‚îÇ
 ‚ñº  POST /api/agent/chat
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ            chi Router                   ‚îÇ
‚îÇ  middleware: Logger ‚Üí CORS              ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  ChatHandler                            ‚îÇ
‚îÇ   ‚îú‚îÄ MaxBytesReader (512 KB limit)      ‚îÇ
‚îÇ   ‚îú‚îÄ json.Decode + validaci√≥n           ‚îÇ
‚îÇ   ‚îú‚îÄ ModalidadToAgent(modalidad)        ‚îÇ
‚îÇ   ‚îî‚îÄ Invoker.InvokeAgent(ctx, agent)   ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  Invoker                                ‚îÇ
‚îÇ   ‚îú‚îÄ AgentEnabled check                 ‚îÇ
‚îÇ   ‚îú‚îÄ CircuitBreaker[agentResult].Execute‚îÇ
‚îÇ   ‚îî‚îÄ shared http.Client ‚Üí doHTTP       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
           ‚îÇ
           ‚îú‚îÄ‚îÄCB‚îÄ‚îÄ> Agente Venta   :8001/api/chat
           ‚îú‚îÄ‚îÄCB‚îÄ‚îÄ> Agente Cita    :8002/api/chat
           ‚îú‚îÄ‚îÄCB‚îÄ‚îÄ> Agente Reserva :8003/api/chat
           ‚îî‚îÄ‚îÄCB‚îÄ‚îÄ> Agente CitasV  :8004/api/chat
```

### Capas identificadas

| Capa | Archivo | Responsabilidad |
|---|---|---|
| Entry point | `cmd/gateway/main.go` | Router, server setup, graceful shutdown |
| Config | `internal/config/config.go` | Env vars ‚Üí struct, lookup de URLs/flags |
| Handler | `internal/handler/chat.go` | Decode, validate, orchestrate, respond |
| Health | `internal/handler/health.go` | Health check compuesto del gateway + agentes |
| Proxy | `internal/proxy/agents.go` | HTTP call al agente + circuit breaker |
| Middleware | `internal/middleware/` | CORS, logging |
| Metrics | `internal/metrics/metrics.go` | Prometheus counters + histogramas |

**Evaluaci√≥n de capas:** La separaci√≥n es correcta. El √∫nico punto de acoplamiento menor es que `ChatHandler` depende del tipo concreto `*proxy.Invoker` en lugar de una interfaz ‚Äî correcci√≥n sencilla pero que facilitar√≠a testing unitario.

### Stack tecnol√≥gico

| Componente | Tecnolog√≠a |
|---|---|
| Router | Chi v5 |
| Config | cleanenv (env ‚Üí struct) |
| Logging | `log/slog` stdlib (JSON estructurado) |
| M√©tricas | `prometheus/client_golang` |
| Circuit Breaker | `sony/gobreaker v2` (por agente) |
| HTTP Client | `net/http.Client` (pool compartido) |

---

## 3. Hallazgos y Problemas

---

### üî¥ Cr√≠tico

---

#### C1 ‚Äî Race condition WriteTimeout vs AgentTimeout: goroutines zombies

**Archivos:** `cmd/gateway/main.go` + `internal/proxy/agents.go`

**Descripci√≥n:** El servidor tiene `WriteTimeout=30s` y el cliente de agentes tiene `AGENT_TIMEOUT=30s`. Ambos iguales es el **peor escenario posible**.

**Secuencia de fallo (escenario 1 ‚Äî agente lento):**

```
t=0s    n8n env√≠a request
t=0s    handler goroutine inicia ‚Üí llama doHTTP(ctx = r.Context())
t=30s   WriteTimeout dispara ‚Üí servidor cancela el write deadline de la conexi√≥n
        n8n recibe "connection reset" o "504 gateway timeout"
t=30s   r.Context() NO se cancela (WriteTimeout NO cancela el request context)
t=30s   El agente responde justo en ese momento ‚Üí doHTTP recibe respuesta
t=30s   handler intenta json.Encode(w) ‚Üí "write: broken pipe"
        Goroutine retorna, pero recursos del agente fueron desperdiciados
```

**Secuencia de fallo (escenario 2 ‚Äî agente colgado):**

```
t=0s    handler inicia, llama doHTTP
t=30s   WriteTimeout dispara ‚Üí n8n ya desconectado
t=30s   Agente a√∫n sin responder
t=30s   Goroutine SIGUE VIVA esperando al agente
t=30s+  http.Client.Timeout dispara (desde que se llam√≥ Do()) ‚Üí doHTTP retorna error
t=30s+  handler intenta escribir fallback ‚Üí "write: broken pipe"
        Goroutine zombi durante varios segundos adicionales post-WriteTimeout
```

**Causa ra√≠z:** `r.Context()` en `net/http` de Go **no se cancela autom√°ticamente cuando `WriteTimeout` dispara**. Solo se cancela cuando:
1. El cliente cierra la conexi√≥n activamente.
2. `http.Server.Shutdown()` es llamado.

Como el handler pasa `r.Context()` directamente a `http.NewRequestWithContext(ctx, ...)`, no hay ning√∫n mecanismo que aborte la llamada al agente cuando el cliente (n8n) ya desconect√≥.

**Fix ‚Äî agregar contexto acotado expl√≠citamente en el handler:**

```go
// internal/handler/chat.go ‚Äî en ServeHTTP, antes de InvokeAgent
agentTimeout := time.Duration(h.Invoker.AgentTimeoutSec()) * time.Second
agentCtx, cancel := context.WithTimeout(r.Context(), agentTimeout)
defer cancel()

reply, url, err := h.Invoker.InvokeAgent(agentCtx, agent, req.Message, req.SessionID, contextForAgent)
```

**Fix ‚Äî ajustar valores de config para garantizar buffer:**

```env
# Regla: AGENT_TIMEOUT + ~5s buffer < GATEWAY_WRITE_TIMEOUT_SEC
AGENT_TIMEOUT=25
GATEWAY_WRITE_TIMEOUT_SEC=35
GATEWAY_READ_TIMEOUT_SEC=40
```

Esto garantiza que si el agente tarda demasiado, el contexto lo cancela y el handler tiene 5‚Äì10 segundos para escribir la respuesta de fallback **antes** de que `WriteTimeout` dispare.

---

#### C2 ‚Äî http.Transport sin `ResponseHeaderTimeout` ni `MaxConnsPerHost`

**Archivo:** `internal/proxy/agents.go:30-38`

**C√≥digo actual:**

```go
Transport: &http.Transport{
    MaxIdleConns:        50,
    MaxIdleConnsPerHost: 10,
    IdleConnTimeout:     90 * time.Second,
}
```

**Problema 1 ‚Äî Sin `ResponseHeaderTimeout`:** Un agente puede aceptar la conexi√≥n TCP, enviar el status line `HTTP/1.1 200 OK`, y luego **nunca enviar los headers de respuesta**. El goroutine queda bloqueado hasta que `http.Client.Timeout` expire (30s). Durante ese tiempo: socket abierto, goroutine ocupada, y el agente malgast√≥ una conexi√≥n.

**Problema 2 ‚Äî Sin `MaxConnsPerHost`:** Bajo alta concurrencia, Go puede abrir **conexiones TCP ilimitadas** hacia el mismo agente. 200 requests simult√°neas = 200 conexiones TCP al mismo host. Esto puede saturar el agente Python (FastAPI/Flask con workers limitados) antes de que el circuit breaker tenga tiempo de reaccionar.

**Problema 3 ‚Äî Sin `DialContext` con timeout:** Si el agente est√° ca√≠do pero el host responde al TCP SYN con RST, la conexi√≥n falla r√°pido. Pero si el host no responde (firewall drop), la conexi√≥n espera el timeout de TCP del OS (~2 minutos) en lugar del timeout configurado.

**Fix ‚Äî Transport completo y correctamente configurado:**

```go
import "net"

func newTransport(cfg *config.Config) *http.Transport {
    return &http.Transport{
        // Dialer TCP con timeout expl√≠cito
        DialContext: (&net.Dialer{
            Timeout:   5 * time.Second,  // falla r√°pido si el agente no responde TCP
            KeepAlive: 30 * time.Second, // mantiene conexiones vivas entre requests
        }).DialContext,

        // L√≠mites de conexiones
        MaxConnsPerHost:     25,  // m√°x. conexiones activas por host (backpressure TCP)
        MaxIdleConnsPerHost: 10,  // conexiones idle en el pool por host
        MaxIdleConns:        50,  // total idle en el pool global
        IdleConnTimeout:     90 * time.Second,

        // Timeouts granulares a nivel de transporte
        TLSHandshakeTimeout:   5 * time.Second,
        ResponseHeaderTimeout: 20 * time.Second, // CR√çTICO: el agente debe enviar headers en 20s
        ExpectContinueTimeout: 1 * time.Second,

        // HTTP/1.1 (los agentes Python probablemente no soportan HTTP/2)
        ForceAttemptHTTP2: false,
        DisableKeepAlives: false, // keepalive = reutilizar conexiones TCP (siempre activado)
    }
}
```

---

#### C3 ‚Äî CORS: `Allow-Credentials: true` siempre activo, incluso con `Origin: *`

**Archivo:** `internal/middleware/cors.go:20-22`

**C√≥digo actual:**

```go
// Se env√≠a siempre, sin importar si el origin es wildcard o espec√≠fico
w.Header().Set("Access-Control-Allow-Credentials", "true")
```

**Problema 1 ‚Äî Violaci√≥n del est√°ndar CORS:** Cuando `CORS_ALLOWED_ORIGINS=*` (el default actual del `.env`), la combinaci√≥n `Access-Control-Allow-Origin: *` + `Access-Control-Allow-Credentials: true` es **inv√°lida seg√∫n la especificaci√≥n CORS**. Los browsers modernos rechazan silenciosamente estas respuestas ‚Äî cualquier llamada desde un browser fallar√° sin mensaje de error claro.

**Problema 2 ‚Äî Riesgo en producci√≥n con origins espec√≠ficos:** Cuando en producci√≥n se configure `CORS_ALLOWED_ORIGINS=https://app.maravia.pe`, el header `Allow-Credentials: true` habilita que ese origen haga requests **credencializados** (con cookies, authorization headers) sin ninguna validaci√≥n de autenticidad del request. Esto ampl√≠a la superficie de ataque si el gateway no tiene autenticaci√≥n propia.

**Fix:**

```go
func CORS(origins string) func(http.Handler) http.Handler {
    allowed := strings.Split(origins, ",")
    for i := range allowed {
        allowed[i] = strings.TrimSpace(allowed[i])
    }
    return func(next http.Handler) http.Handler {
        return http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {
            origin := r.Header.Get("Origin")
            allowWildcard := false

            if origin != "" {
                for _, o := range allowed {
                    if o == "*" {
                        w.Header().Set("Access-Control-Allow-Origin", "*")
                        allowWildcard = true
                        break
                    } else if o == origin {
                        w.Header().Set("Access-Control-Allow-Origin", origin)
                        w.Header().Set("Vary", "Origin") // necesario para caches
                        break
                    }
                }
            }

            // Allow-Credentials solo cuando el origin es espec√≠fico (nunca con wildcard)
            if !allowWildcard {
                w.Header().Set("Access-Control-Allow-Credentials", "true")
            }

            w.Header().Set("Access-Control-Allow-Methods", "GET, POST, OPTIONS")
            w.Header().Set("Access-Control-Allow-Headers", "Content-Type, Accept, X-Request-ID")

            if r.Method == http.MethodOptions {
                w.WriteHeader(http.StatusNoContent)
                return
            }
            next.ServeHTTP(w, r)
        })
    }
}
```

---

### üü° Medio

---

#### M1 ‚Äî Sin backpressure: goroutines acumulables bajo carga

**Descripci√≥n:** No hay l√≠mite de concurrencia hacia los agentes. Ante un pico de 200 requests simult√°neas:

- 200 goroutines activas, cada una esperando respuesta del agente (~8 KB por goroutine = ~1.6 MB m√≠nimo, m√°s buffers).
- Sin `MaxConnsPerHost` (ver C2), Go abre 200 conexiones TCP al agente.
- El agente Python se satura ‚Üí latencias crecientes ‚Üí m√°s goroutines acumuladas ‚Üí **cascada de fallos**.

El circuit breaker solo act√∫a despu√©s de 5 fallos consecutivos. Mientras tanto, los goroutines se acumulan.

**Fix ‚Äî sem√°foro por agente (backpressure limpio):**

```go
// internal/proxy/agents.go

type Invoker struct {
    cfg    *config.Config
    client *http.Client
    cbs    map[string]*gobreaker.CircuitBreaker[agentResult]
    sems   map[string]chan struct{} // limitador de concurrencia por agente
}

// En NewInvoker, inicializar sem√°foros:
sems := make(map[string]chan struct{}, len(agents))
for _, name := range agents {
    sems[name] = make(chan struct{}, 20) // m√°x. 20 requests concurrentes por agente
}

// En InvokeAgent, antes de llamar al circuit breaker:
sem, ok := inv.sems[agent]
if ok {
    select {
    case sem <- struct{}{}: // adquirir slot
        defer func() { <-sem }() // liberar al terminar
    default:
        // Backpressure: agente saturado, rechazar inmediatamente
        return "", nil, fmt.Errorf("agent %s: demasiadas requests concurrentes (backpressure)", agent)
    }
}
```

En el handler, mapear este error a **HTTP 503** (no al fallback gen√©rico):

```go
if errors.Is(err, ErrBackpressure) {
    w.WriteHeader(http.StatusServiceUnavailable)
    writeJSON(w, http.StatusServiceUnavailable, map[string]string{
        "detail": "Servicio temporalmente saturado. Intenta en un momento.",
    })
    return
}
```

---

#### M2 ‚Äî Circuit breaker tarda demasiado en abrir (150 segundos de fallos lentos)

**Configuraci√≥n actual:**

```go
ReadyToTrip: func(counts gobreaker.Counts) bool {
    return counts.ConsecutiveFailures >= 5 // ‚Üê umbral alto
},
Timeout:  60 * time.Second, // tiempo en estado abierto antes de probar
Interval: 60 * time.Second,
```

**Problema:** Con `ConsecutiveFailures >= 5` y `AGENT_TIMEOUT=30s`:

```
Fallo 1 ‚Üí espera 30s ‚Üí error
Fallo 2 ‚Üí espera 30s ‚Üí error
Fallo 3 ‚Üí espera 30s ‚Üí error
Fallo 4 ‚Üí espera 30s ‚Üí error
Fallo 5 ‚Üí espera 30s ‚Üí error ‚Üí BREAKER ABRE
Total: 150 segundos de requests fallando lentamente hacia n8n
```

Durante esos 150 segundos, n8n recibe timeouts o errores, y las goroutines se acumulan.

**Fix:**

```go
cbs[name] = gobreaker.NewCircuitBreaker[agentResult](gobreaker.Settings{
    Name:        name,
    MaxRequests: 3,
    Interval:    60 * time.Second,
    Timeout:     30 * time.Second, // probar recuperaci√≥n cada 30s (era 60s)
    ReadyToTrip: func(counts gobreaker.Counts) bool {
        return counts.ConsecutiveFailures >= 3 // abrir m√°s r√°pido (era 5)
    },
    OnStateChange: func(name string, from, to gobreaker.State) {
        slog.Warn("circuit_breaker_state_change",
            "agent", name,
            "from", from.String(),
            "to", to.String(),
        )
        // Actualizar m√©trica Prometheus (ver secci√≥n Observabilidad)
        metrics.CircuitBreakerState.WithLabelValues(name).Set(float64(to))
    },
})
```

Con `AgentTimeout=25s` y umbral=3: m√°ximo **75 segundos** antes de que el breaker abra (vs 150s actuales).

---

#### M3 ‚Äî Sin retries para errores de red transitorios

**Descripci√≥n:** Un flap de red transitorio o restart del agente Python causa fallo inmediato sin ning√∫n reintento. Para este caso (chatbot con `session_id`), un retry es seguro porque el agente puede manejar mensajes repetidos.

**Fix ‚Äî 1 retry con backoff m√≠nimo para errores de red:**

```go
func isRetryableError(err error) bool {
    // Solo reintentar errores de conexi√≥n/red, no errores de aplicaci√≥n
    var netErr net.Error
    if errors.As(err, &netErr) {
        return netErr.Timeout() || !netErr.Temporary()
    }
    return strings.Contains(err.Error(), "connection refused") ||
           strings.Contains(err.Error(), "connection reset")
}

func (inv *Invoker) doHTTPWithRetry(ctx context.Context, agentURL string, ...) (agentResult, error) {
    var lastErr error
    for attempt := 0; attempt < 2; attempt++ {
        if attempt > 0 {
            // Backoff m√≠nimo, respetando el contexto padre
            select {
            case <-ctx.Done():
                return agentResult{}, ctx.Err()
            case <-time.After(500 * time.Millisecond):
            }
            slog.Debug("retrying agent call", "url", agentURL, "attempt", attempt+1)
        }
        res, err := inv.doHTTP(ctx, agentURL, ...)
        if err == nil {
            return res, nil
        }
        if isRetryableError(err) {
            lastErr = err
            continue
        }
        return agentResult{}, err // error no retriable ‚Üí fallo inmediato
    }
    return agentResult{}, fmt.Errorf("after %d attempts: %w", 2, lastErr)
}
```

---

#### M4 ‚Äî Sin X-Request-ID ni correlation ID

**Descripci√≥n:** No se genera ni propaga ning√∫n ID de correlaci√≥n. Es imposible correlacionar un log del gateway con el log correspondiente del agente Python. Si n8n env√≠a un `X-Request-ID`, se descarta silenciosamente.

**Fix ‚Äî middleware de correlation ID:**

```go
// internal/middleware/request_id.go

type contextKey string
const RequestIDKey contextKey = "request_id"

func RequestID(next http.Handler) http.Handler {
    return http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {
        id := r.Header.Get("X-Request-ID")
        if id == "" {
            // Generar un ID √∫nico si n8n no lo env√≠a
            id = fmt.Sprintf("%d-%d", time.Now().UnixNano(), rand.Int63())
        }
        ctx := context.WithValue(r.Context(), RequestIDKey, id)
        w.Header().Set("X-Request-ID", id) // devolver al cliente para trazabilidad
        next.ServeHTTP(w, r.WithContext(ctx))
    })
}

func GetRequestID(ctx context.Context) string {
    if id, ok := ctx.Value(RequestIDKey).(string); ok {
        return id
    }
    return ""
}
```

Agregar al router antes de CORS:

```go
r.Use(middleware.RequestID)
r.Use(middleware.Logger)
r.Use(middleware.CORS(cfg.CORSOrigins))
```

Propagar al agente en `doHTTP`:

```go
req.Header.Set("X-Request-ID", middleware.GetRequestID(ctx))
```

Incluir en todos los logs del handler y proxy:

```go
slog.Info("‚Üí request entrada",
    "request_id", middleware.GetRequestID(r.Context()),
    "modalidad", req.Config.Modalidad,
    // ...
)
```

---

#### M5 ‚Äî config.Load() no carga el archivo .env en desarrollo local

**Archivo:** `internal/config/config.go:22`

**C√≥digo actual:**

```go
_ = cleanenv.ReadEnv(nil) // ‚Üê no hace nada √∫til
var c Config
if err := cleanenv.ReadEnv(&c); err != nil { ... }
```

**Problema:** `cleanenv.ReadEnv` lee variables **del entorno del proceso** (variables ya exportadas), no de un archivo `.env`. La llamada con `nil` es un no-op. En Docker Compose con `env_file: .env`, las variables ya est√°n inyectadas en el entorno ‚Üí funciona. Pero al ejecutar `go run ./cmd/gateway` directamente en desarrollo local, **el `.env` no se carga autom√°ticamente**.

**Fix:**

```go
func Load() (*Config, error) {
    // Intentar cargar .env si existe (solo para desarrollo local; en Docker ya est√° en el entorno)
    if err := godotenv.Load(); err != nil && !os.IsNotExist(err) {
        // No es error cr√≠tico si .env no existe (producci√≥n sin archivo)
        slog.Debug("config: .env not found, using environment variables only")
    }
    var c Config
    if err := cleanenv.ReadEnv(&c); err != nil {
        return nil, fmt.Errorf("config: %w", err)
    }
    // ...
}
```

---

#### M6 ‚Äî ModalidadToAgent: fallback silencioso a "cita"

**Archivo:** `internal/proxy/agents.go`

**C√≥digo actual:**

```go
default:
    return "cita" // ‚Üê silencioso, sin warning
```

**Problema:** Si n8n env√≠a una modalidad incorrecta (`"ventas"` con min√∫scula, `"CITAS"` con may√∫scula, un typo, etc.), el gateway lo rutea silenciosamente al agente de citas. En producci√≥n esto causa que conversaciones de ventas lleguen al agente de citas sin ning√∫n aviso en los logs.

**Fix ‚Äî loguear como warning y opcionalmente rechazar:**

```go
func ModalidadToAgent(modalidad string) string {
    m := strings.ToLower(strings.TrimSpace(modalidad))
    switch m {
    case "citas":
        return "cita"
    case "ventas":
        return "venta"
    case "reservas":
        return "reserva"
    case "citas y ventas":
        return "citas_ventas"
    default:
        slog.Warn("modalidad desconocida, usando fallback",
            "modalidad_recibida", modalidad,
            "modalidad_normalizada", m,
            "fallback", "cita",
        )
        return "cita"
    }
}
```

Alternativa m√°s estricta: retornar un error y responder HTTP 400 al cliente, forzando a n8n a configurar correctamente el campo.

---

#### M7 ‚Äî logStartup usa fmt.Sprintf en slog: rompe el structured logging

**Archivo:** `cmd/gateway/main.go`

**C√≥digo actual:**

```go
slog.Info(fmt.Sprintf("  Host         : %s", addr))
slog.Info(fmt.Sprintf("  Go version   : %s", runtime.Version()))
```

**Problema:** Esto produce logs con un √∫nico campo de mensaje string, imposibles de parsear por herramientas como Grafana Loki, Datadog, Elastic o cualquier sistema de log estructurado. El prop√≥sito completo de `slog` es mantener key-value pairs separados.

**Fix:**

```go
slog.Info("gateway_startup",
    "addr", addr,
    "go_version", runtime.Version(),
    "log_level", cfg.LogLevel,
    "cors_origins", cfg.CORSOrigins,
    "agent_timeout_sec", cfg.AgentTimeoutSec,
    "read_header_timeout_sec", cfg.ReadHeaderTimeoutSec,
    "write_timeout_sec", cfg.WriteTimeoutSec,
    "agents_enabled", enabledAgents(cfg),
)
```

---

### üü¢ Mejora

---

#### G1 ‚Äî M√©tricas Prometheus insuficientes para diagn√≥stico de producci√≥n

**Estado actual:** solo `gateway_requests_total` y `gateway_request_duration_seconds`.

**Faltan:**

```go
// internal/metrics/metrics.go ‚Äî adiciones

// Requests en vuelo en este momento (gauge)
var InFlightRequests = promauto.NewGaugeVec(
    prometheus.GaugeOpts{
        Name: "gateway_inflight_requests",
        Help: "Requests activos en vuelo por agente",
    },
    []string{"agent"},
)

// Estado del circuit breaker por agente (0=closed, 1=open, 2=half-open)
var CircuitBreakerState = promauto.NewGaugeVec(
    prometheus.GaugeOpts{
        Name: "gateway_circuit_breaker_state",
        Help: "Estado del circuit breaker por agente (0=closed, 1=open, 2=half-open)",
    },
    []string{"agent"},
)

// HTTP status del upstream por agente
var UpstreamStatusTotal = promauto.NewCounterVec(
    prometheus.CounterOpts{
        Name: "gateway_upstream_http_status_total",
        Help: "Respuestas HTTP del agente upstream por c√≥digo de status",
    },
    []string{"agent", "status_code"},
)

// Tipo de error del agente
var AgentErrorTotal = promauto.NewCounterVec(
    prometheus.CounterOpts{
        Name: "gateway_agent_error_total",
        Help: "Errores por tipo: timeout, connection_refused, circuit_open, decode_error",
    },
    []string{"agent", "error_type"},
)
```

**Uso en InvokeAgent:**

```go
metrics.InFlightRequests.WithLabelValues(agent).Inc()
defer metrics.InFlightRequests.WithLabelValues(agent).Dec()
```

---

#### G2 ‚Äî Health check hace requests s√≠ncronos secuenciales

**Archivo:** `internal/handler/health.go`

**Problema:** Con 4 agentes ca√≠dos, cada check espera 2s (timeout) antes de pasar al siguiente. El endpoint `/health` tarda hasta **8 segundos** en responder. Cualquier load balancer con un health check timeout de 1‚Äì2s marcar√° el gateway como ca√≠do.

**Fix ‚Äî checks paralelos con WaitGroup:**

```go
func (h *HealthHandler) checkAgents() (map[string]string, bool) {
    agentNames := []string{"venta", "cita", "reserva", "citas_ventas"}
    results := make(map[string]string, len(agentNames))
    var mu sync.Mutex
    var wg sync.WaitGroup
    allOK := true

    for _, name := range agentNames {
        wg.Add(1)
        go func(n string) {
            defer wg.Done()
            status := h.checkAgent(n)
            mu.Lock()
            results[n] = status
            if status != "ok" && status != "disabled" {
                allOK = false
            }
            mu.Unlock()
        }(name)
    }
    wg.Wait()
    return results, allOK
}
```

Con esta implementaci√≥n, el timeout m√°ximo de `/health` es **2 segundos** (un solo check en paralelo), no 8.

---

#### G3 ‚Äî Graceful shutdown timeout insuficiente para requests en vuelo

**Archivo:** `cmd/gateway/main.go`

**C√≥digo actual:**

```go
ctx, cancel := context.WithTimeout(context.Background(), 10*time.Second)
```

**Problema:** Con `AGENT_TIMEOUT=30s`, si al momento del shutdown hay goroutines esperando respuesta del agente, `srv.Shutdown(10s)` forzar√° el cierre despu√©s de 10 segundos. Las requests en vuelo se cortar√°n abruptamente, dejando a n8n sin respuesta.

**Fix:** El shutdown timeout debe ser al menos `AGENT_TIMEOUT + un buffer`:

```go
shutdownTimeout := time.Duration(cfg.AgentTimeoutSec+10) * time.Second
ctx, cancel := context.WithTimeout(context.Background(), shutdownTimeout)
```

---

#### G4 ‚Äî Sin autenticaci√≥n de requests entrantes

**Descripci√≥n:** Cualquier proceso que conozca la IP:puerto del gateway puede enviar requests arbitrarios a `/api/agent/chat`. Si el gateway est√° expuesto a internet (incluso detr√°s de un proxy), esto es un vector de abuso y costos.

**Fix m√≠nimo ‚Äî middleware de API key:**

```go
// internal/middleware/auth.go
func APIKey(validKey string) func(http.Handler) http.Handler {
    return func(next http.Handler) http.Handler {
        return http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {
            if validKey == "" {
                // Sin configurar ‚Üí pasar (permite desactivar en dev)
                next.ServeHTTP(w, r)
                return
            }
            key := r.Header.Get("X-API-Key")
            if key == "" {
                key = r.URL.Query().Get("api_key") // fallback para n8n query param
            }
            if key != validKey {
                http.Error(w, `{"detail":"unauthorized"}`, http.StatusUnauthorized)
                return
            }
            next.ServeHTTP(w, r)
        })
    }
}
```

Agregar variable de entorno:

```env
GATEWAY_API_KEY=tu-clave-secreta-aqui
```

---

#### G5 ‚Äî responseWriter wrapper no implementa http.Flusher

**Archivo:** `internal/middleware/logger.go`

**C√≥digo actual:**

```go
type responseWriter struct {
    http.ResponseWriter
    status int
}
```

**Problema:** Si alg√∫n handler usa `http.Flusher` para streaming, la type assertion `w.(http.Flusher)` falla silenciosamente porque el wrapper no implementa esa interfaz. Aunque no es un problema hoy (respuestas s√≠ncronas), es una deuda t√©cnica.

**Fix:**

```go
func (w *responseWriter) Flush() {
    if f, ok := w.ResponseWriter.(http.Flusher); ok {
        f.Flush()
    }
}
```

---

#### G6 ‚Äî .env y .env.example desincronizados

**Problema:** `.env` no incluye `AGENT_CITAS_VENTAS_URL` ni `AGENT_CITAS_VENTAS_ENABLED`, pero el c√≥digo s√≠ soporta el agente `citas_ventas`. En runtime se usan los defaults del struct (`http://localhost:8004/api/chat`, `true`), pero no est√° documentado en `.env`.

**Fix:** Sincronizar `.env` con `.env.example`:

```env
AGENT_CITAS_VENTAS_URL=http://localhost:8004/api/chat
AGENT_CITAS_VENTAS_ENABLED=true
```

---

## 4. Riesgos de Producci√≥n

| # | Riesgo | Escenario que lo activa | Impacto | Probabilidad |
|---|---|---|---|---|
| R1 | **Goroutines zombies** | Agente lento (‚â•30s), WriteTimeout dispara, goroutine sigue viva | Memory leak gradual, OOM | Alta |
| R2 | **Cascada de fallos** | 1 agente lento satura el pool ‚Üí latencias globales suben | Degradaci√≥n total del servicio | Media |
| R3 | **TCP socket exhaustion** | Alta carga sin `MaxConnsPerHost` | Agente saturado, `connection refused` | Media |
| R4 | **Breaker tarda en abrir** | 5 fallos √ó 30s = 150s de requests lentas antes de apertura | n8n timeouts en cascada | Alta |
| R5 | **Health check lento** | 4 agentes ca√≠dos ‚Üí 8s para responder `/health` | LB/orchestrator marca gateway como muerto | Media |
| R6 | **CORS bug** | Browser hace requests (UI futura) con wildcard + credentials | Requests silenciosamente rechazadas por browser | Baja-Media |
| R7 | **Sin autenticaci√≥n** | Endpoint accesible desde red no confiable | Abuso, costos de agentes, spam | Depende de red |
| R8 | **Modalidad silenciosa** | n8n env√≠a modalidad incorrecta | Ventas ruteadas a Citas sin aviso | Media |
| R9 | **Shutdown brusco** | Requests en vuelo durante deploy/restart | n8n recibe error en mitad de conversaci√≥n | Media |

### Qu√© falla primero bajo carga

```
Escenario: tr√°fico creciente de n8n

1. Primero: Agente Python se satura (workers limitados, I/O bound con LLM)
   ‚Üí Latencias suben de 3s ‚Üí 15s ‚Üí 30s

2. Segundo: Goroutines del gateway se acumulan (cada request bloqueada 30s)
   ‚Üí Memoria del gateway sube
   ‚Üí CPU sube por GC pressure

3. Tercero: Sin MaxConnsPerHost, el pool TCP del agente se agota
   ‚Üí "connection refused" o "too many open files"

4. Cuarto: Sin backpressure (sem√°foro), el gateway no rechaza requests nuevas
   ‚Üí M√°s goroutines ‚Üí m√°s memoria ‚Üí OOM o crash

5. El circuit breaker abre TARDE (150s) ‚Üí durante ese tiempo todo est√° degradado
```

---

## 5. Optimizaci√≥n de Performance

### http.Transport ‚Äî Configuraci√≥n Completa Recomendada

```go
// internal/proxy/agents.go

import "net"

func newTransport() *http.Transport {
    return &http.Transport{
        // Dialer TCP con timeout expl√≠cito
        DialContext: (&net.Dialer{
            Timeout:   5 * time.Second,  // falla r√°pido si el agente no responde TCP
            KeepAlive: 30 * time.Second, // mantiene conexiones vivas entre requests
        }).DialContext,

        // L√≠mites de conexiones (ajustar seg√∫n carga esperada)
        MaxConnsPerHost:     25,  // m√°x. activas por host ‚Äî previene saturar el agente
        MaxIdleConnsPerHost: 10,  // pool de conexiones reutilizables por host
        MaxIdleConns:        50,  // total de conexiones idle en todo el pool
        IdleConnTimeout:     90 * time.Second,

        // Timeouts granulares
        TLSHandshakeTimeout:   5 * time.Second,
        ResponseHeaderTimeout: 20 * time.Second, // el agente debe enviar headers en 20s
        ExpectContinueTimeout: 1 * time.Second,

        // HTTP/1.1 con keepalive (adecuado para agentes Python)
        ForceAttemptHTTP2: false,
        DisableKeepAlives: false, // reutilizar conexiones TCP = menos overhead
    }
}
```

### Timeouts end-to-end correctamente encadenados

```
n8n ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Gateway ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Agente
     ReadHeaderTimeout(10s)   WriteTimeout(35s)            AgentTimeout(25s)
     ‚óÑ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫     ‚óÑ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫         ‚óÑ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫
                              ‚óÑ‚îÄ‚îÄ ResponseHeaderTimeout(20s) ‚îÄ‚îÄ‚ñ∫
                              ‚óÑ‚îÄ‚îÄ DialTimeout(5s) ‚îÄ‚îÄ‚ñ∫

Regla de oro:
  DialTimeout(5s)                          < ResponseHeaderTimeout(20s)
  ResponseHeaderTimeout(20s)               < AgentTimeout(25s)
  AgentTimeout(25s) + buffer(5-10s)        < WriteTimeout(35s)
  WriteTimeout(35s)                        < ReadTimeout(40s)
  ReadTimeout(40s)                        <= IdleTimeout(60s)
```

### ¬øHTTP/1.1, HTTP/2, gRPC o WebSocket?

| Protocolo | Ventaja | Cu√°ndo usarlo |
|---|---|---|
| **HTTP/1.1 + keepalive** (actual) | Simple, universal, compatible con FastAPI/Flask | Ahora ‚Äî mantener |
| **HTTP/2** | Multiplexing (varios streams por TCP), header compression | Cuando los agentes lo soporten y haya >20 req/s por agente |
| **gRPC** | Contrato fuerte (protobuf), streaming bidireccional, mejor performance | Si se redise√±an los agentes en Go o si se requiere streaming real |
| **WebSocket** | Bidireccional, eventos push al usuario | Solo si el usuario final necesita respuestas en streaming real-time |

**Recomendaci√≥n:** Quedarse en HTTP/1.1 con el Transport mejorado. gRPC es el roadmap ideal para cuando los agentes sean propios y controlados.

### ¬øSe necesita una cola (Redis/NATS)?

**No, para el caso de uso actual.** El gateway es un proxy s√≠ncrono: n8n espera la respuesta del agente. Una cola a√±adir√≠a latencia y complejidad sin beneficio neto.

Lo que **s√≠ se necesita** (y es m√°s simple): el **sem√°foro de concurrencia por agente** (ver M1) para backpressure.

**Cu√°ndo reconsiderar una cola:**
- Si n8n no puede esperar 25‚Äì30 segundos y se requiere procesamiento as√≠ncrono con webhook de vuelta.
- Si se requiere durabilidad de mensajes (reintentos persistentes ante crash).
- Si el volumen supera los cientos de requests/segundo y se quiere desacoplar completamente producci√≥n de consumo.

### Respuestas grandes: buffers en memoria

```go
// Actual: lee todo el body del agente en memoria con json.Decode
// OK para respuestas de chatbot (<10 KB t√≠picamente)

// Preventivo: agregar LimitReader antes del Decode para proteger contra agentes que devuelvan cuerpos enormes
limitedBody := io.LimitReader(resp.Body, 10*1024*1024) // 10 MB m√°ximo
if err := json.NewDecoder(limitedBody).Decode(&out); err != nil {
    return agentResult{}, fmt.Errorf("decode response: %w", err)
}
```

Para un chatbot de texto esto es m√°s que suficiente. Streaming real (`io.Copy` directo) solo ser√≠a necesario si los agentes devuelven archivos o respuestas multi-MB.

---

## 6. Resiliencia

### Tabla de estado actual vs recomendado

| Mecanismo | Estado actual | Acci√≥n recomendada |
|---|---|---|
| HTTP server ReadHeaderTimeout | ‚úÖ 10s | Mantener |
| HTTP server ReadTimeout | ‚úÖ 30s | Subir a 40s |
| HTTP server WriteTimeout | ‚úÖ 30s | Subir a 35s |
| HTTP server IdleTimeout | ‚úÖ 60s | Mantener |
| http.Client.Timeout | ‚úÖ 30s | Bajar a 25s (< WriteTimeout) |
| context.WithTimeout en handler | ‚ùå Falta | Agregar (C1 ‚Äî cr√≠tico) |
| ResponseHeaderTimeout | ‚ùå Falta | Agregar 20s en Transport (C2) |
| DialTimeout | ‚ùå Falta | Agregar 5s en DialContext (C2) |
| MaxConnsPerHost | ‚ùå Falta | Agregar 25 (C2) |
| Circuit breaker por agente | ‚úÖ gobreaker | Bajar umbral a 3, timeout a 30s (M2) |
| Backpressure / sem√°foro | ‚ùå Falta | Implementar (M1) |
| Retry con backoff | ‚ùå Falta | 1 retry para errores de red (M3) |
| Rate limiting de entrada | ‚ùå Falta | Considerar golang.org/x/time/rate |
| Body drain antes de Close | ‚ö†Ô∏è Impl√≠cito | Agregar io.Copy(io.Discard, resp.Body) en rutas de error |
| Graceful shutdown | ‚úÖ 10s | Aumentar a AgentTimeout+10s (G3) |

### Flujo de estados del circuit breaker (corregido)

```
Closed ‚îÄ(3 fallos consecutivos)‚îÄ‚îÄ‚ñ∫ Open ‚îÄ(30s)‚îÄ‚îÄ‚ñ∫ Half-Open
  ‚ñ≤                                                    ‚îÇ
  ‚îÇ                                              (√©xito en probe)
  ‚îÇ‚óÑ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

  Half-Open ‚îÄ(fallo en probe)‚îÄ‚îÄ‚ñ∫ Open
```

### Body drain y keepalive

Para que HTTP keepalive funcione correctamente (reutilizaci√≥n de conexiones TCP), el body de la respuesta del upstream debe ser completamente drenado antes de cerrarlo:

```go
// En rutas donde se detecta un error ANTES de leer el body completo:
defer func() {
    _, _ = io.Copy(io.Discard, resp.Body) // drenar para permitir keepalive
    resp.Body.Close()
}()
```

Cuando `json.Decode` lee el body completamente (caso normal), esto ya se cumple. El `defer` expl√≠cito protege las rutas de error donde se retorna antes de completar la lectura.

---

## 7. Observabilidad

### Estado actual

| Componente | Estado | Detalle |
|---|---|---|
| Logging estructurado | ‚úÖ slog JSON | Bueno, con previews de mensajes |
| M√©tricas Prometheus | ‚ö†Ô∏è B√°sico | Solo requests_total y duration; faltan inflight, circuit state, upstream status |
| Health check compuesto | ‚úÖ Implementado | Verificar paralelismo (ver G2) |
| Correlation ID / X-Request-ID | ‚ùå Falta | Imposible trazar requests cross-service |
| OpenTelemetry tracing | ‚ùå Falta | Sin spans distribuidos |
| Log de startup estructurado | ‚ö†Ô∏è Strings crudos | fmt.Sprintf en slog (ver M7) |

### M√©tricas Prometheus a agregar

```go
// internal/metrics/metrics.go

var InFlightRequests = promauto.NewGaugeVec(
    prometheus.GaugeOpts{
        Name: "gateway_inflight_requests",
        Help: "Requests activos en vuelo por agente",
    },
    []string{"agent"},
)

var CircuitBreakerState = promauto.NewGaugeVec(
    prometheus.GaugeOpts{
        Name: "gateway_circuit_breaker_state",
        Help: "Estado del circuit breaker (0=closed, 1=open, 2=half-open)",
    },
    []string{"agent"},
)

var UpstreamStatusTotal = promauto.NewCounterVec(
    prometheus.CounterOpts{
        Name: "gateway_upstream_http_status_total",
        Help: "Respuestas HTTP del agente upstream por c√≥digo de status",
    },
    []string{"agent", "status_code"},
)

var AgentErrorTotal = promauto.NewCounterVec(
    prometheus.CounterOpts{
        Name: "gateway_agent_error_total",
        Help: "Errores por tipo: timeout, connection_refused, circuit_open, decode_error, backpressure",
    },
    []string{"agent", "error_type"},
)
```

**Uso en InvokeAgent:**

```go
metrics.InFlightRequests.WithLabelValues(agent).Inc()
defer metrics.InFlightRequests.WithLabelValues(agent).Dec()
```

**Uso en OnStateChange del circuit breaker:**

```go
OnStateChange: func(name string, from, to gobreaker.State) {
    slog.Warn("circuit_breaker_state_change",
        "agent", name,
        "from", from.String(),
        "to", to.String(),
    )
    metrics.CircuitBreakerState.WithLabelValues(name).Set(float64(to))
},
```

### OpenTelemetry (roadmap)

Para un gateway de producci√≥n con m√∫ltiples agentes, el tracing distribuido es el mayor salto de observabilidad. Prioridad: baja ahora, alta cuando haya m√°s de 2 agentes activos simult√°neamente.

```go
// Ejemplo b√°sico con go.opentelemetry.io/otel
ctx, span := otel.Tracer("gateway").Start(ctx, "invoke_agent",
    trace.WithAttributes(
        attribute.String("agent.name", agent),
        attribute.Int("session.id", sessionID),
        attribute.String("modalidad", modalidad),
    ),
)
defer span.End()

// Propagar contexto al agente via HTTP headers
otel.GetTextMapPropagator().Inject(ctx, propagation.HeaderCarrier(req.Header))
```

### Alertas Prometheus recomendadas

```yaml
# Ejemplo de alertas para Grafana/Alertmanager

- alert: GatewayCircuitBreakerOpen
  expr: gateway_circuit_breaker_state == 1
  for: 30s
  annotations:
    summary: "Circuit breaker abierto para agente {{ $labels.agent }}"

- alert: GatewayHighErrorRate
  expr: rate(gateway_requests_total{status="error"}[5m]) / rate(gateway_requests_total[5m]) > 0.1
  for: 2m
  annotations:
    summary: "Tasa de errores >10% en los √∫ltimos 5 minutos"

- alert: GatewayHighLatency
  expr: histogram_quantile(0.95, gateway_request_duration_seconds_bucket) > 20
  for: 2m
  annotations:
    summary: "P95 de latencia supera 20 segundos"

- alert: GatewayInFlightSaturation
  expr: gateway_inflight_requests > 15
  for: 1m
  annotations:
    summary: "M√°s de 15 requests en vuelo para agente {{ $labels.agent }}"
```

---

## 8. Checklist de Producci√≥n

### Cr√≠tico ‚Äî Antes de ir a producci√≥n

```
[ ] C1: Agregar context.WithTimeout en ChatHandler (AGENT_TIMEOUT < WRITE_TIMEOUT - 5s)
[ ] C2: Mejorar http.Transport:
        - ResponseHeaderTimeout: 20s
        - MaxConnsPerHost: 25
        - DialContext con Timeout: 5s y KeepAlive: 30s
        - TLSHandshakeTimeout: 5s
[ ] C3: Corregir CORS (no Allow-Credentials con wildcard)
[ ] M1: Implementar sem√°foro de concurrencia por agente (backpressure)
[ ] M2: Bajar circuit breaker: ConsecutiveFailures=3, Timeout=30s
[ ] G6: Sincronizar .env con .env.example (AGENT_CITAS_VENTAS_URL)
[ ] Ajustar LOG_LEVEL=info en .env de producci√≥n
[ ] Restringir CORS_ALLOWED_ORIGINS a dominios reales en producci√≥n
[ ] Ajustar timeouts: AGENT_TIMEOUT=25, GATEWAY_WRITE_TIMEOUT_SEC=35
```

### Importante ‚Äî Primera semana en producci√≥n

```
[ ] M3: Agregar 1 retry con backoff m√≠nimo para errores de red transitorios
[ ] M4: Implementar middleware X-Request-ID y propagarlo al agente
[ ] M6: Agregar warning log en ModalidadToAgent para modalidades desconocidas
[ ] G2: Paralelizar health checks (tiempo m√°x. de /health = 2s, no 8s)
[ ] G3: Aumentar shutdown timeout a AgentTimeout+10s
[ ] G4: Implementar autenticaci√≥n m√≠nima (API key header)
[ ] G1: Agregar m√©tricas: inflight_requests, circuit_breaker_state, upstream_status
[ ] compose.yaml: agregar healthcheck y resource limits (memory, cpu)
```

### Mejoras ‚Äî Sprint posterior

```
[ ] M5: Cargar .env expl√≠citamente en desarrollo (godotenv.Load())
[ ] M7: Refactorizar logStartup a structured logging (sin fmt.Sprintf)
[ ] G5: Implementar http.Flusher en responseWriter
[ ] G1: Agregar m√©tricas de error_type (timeout/connection/circuit/decode)
[ ] Rate limiting de entrada (golang.org/x/time/rate por IP o por cliente)
[ ] OpenTelemetry tracing con propagaci√≥n al agente
[ ] Alertas Prometheus (circuit breaker open, alta latencia, alta tasa de error)
[ ] ChatHandler: usar interfaz en lugar de *proxy.Invoker (facilita testing)
[ ] Tests de integraci√≥n con agente mock
[ ] io.LimitReader en decode de respuesta del agente (10 MB m√°ximo)
```

---

## 9. Score de Madurez

| Dimensi√≥n | Score | Fortalezas | Gaps principales |
|---|---|---|---|
| Arquitectura y separaci√≥n de capas | 8/10 | Layout est√°ndar Go, capas claras | ChatHandler acoplado a tipo concreto |
| Alta concurrencia y backpressure | 4/10 | Goroutines nativas de Go | Sin sem√°foro, sin MaxConnsPerHost, goroutines zombies |
| Uso de net/http y Transport | 6/10 | Cliente compartido ‚úÖ | Transport incompleto (sin ResponseHeaderTimeout, sin DialTimeout) |
| Gesti√≥n de recursos (pool, keepalive) | 5/10 | MaxIdleConns configurado | Sin MaxConnsPerHost, sin l√≠mite activo |
| Resiliencia (timeouts, retry, breaker) | 6/10 | Circuit breaker por agente ‚úÖ | Race condition timeout ‚ùå, sin retry ‚ùå, breaker lento ‚ùå |
| Observabilidad (logs, m√©tricas, tracing) | 5/10 | slog JSON ‚úÖ, Prometheus b√°sico ‚úÖ | Sin X-Request-ID, sin circuit state metrics, sin tracing |
| Seguridad | 5/10 | Body limit ‚úÖ, timeouts ‚úÖ | CORS bug ‚ùå, sin auth ‚ùå, wildcard CORS en producci√≥n ‚ùå |
| Correctitud del c√≥digo | 7/10 | FlexBool/FlexInt bien resueltos, ctx propagado | WriteTimeout race condition |
| **TOTAL ACTUAL** | **6.2 / 10** | Base s√≥lida para MVP | Gaps reales para producci√≥n bajo carga |

### Proyecci√≥n tras aplicar fixes

| Fase | Fixes aplicados | Score esperado |
|---|---|---|
| Ahora (MVP) | Ninguno | 6.2 / 10 |
| Cr√≠ticos (C1, C2, C3, M1, M2) | Race condition, Transport, CORS, backpressure, breaker | ~7.8 / 10 |
| Importantes (M3-M6, G2-G4, G6) | Retry, correlaci√≥n, auth, health paralelo | ~8.5 / 10 |
| Mejoras (G1, G5, tracing, tests) | M√©tricas completas, OTel, testing | ~9.0 / 10 |

---

> **Conclusi√≥n:** El c√≥digo es limpio, idiom√°tico en Go y bien estructurado para un proyecto de tama√±o peque√±o-mediano. Los problemas identificados no son de dise√±o fundamental, sino de configuraci√≥n y mecanismos de resiliencia que se agregan iterativamente. Los dos fixes m√°s urgentes son **C1** (goroutines zombies por race de timeouts) y **C2** (Transport incompleto), ya que son los √∫nicos que pueden causar degradaci√≥n catastr√≥fica bajo carga real. El resto del sistema seguir√° funcional sin ellos, pero con riesgo creciente a medida que escale el tr√°fico.
